# Story 2.1: Enhance Summarization Agent for Debate Format

## Status
Done

## Story
**As a** Skeptical Verifier user,
**I want** the summarization agent to generate multi-perspective debate format summaries,
**so that** I can see different viewpoints on news topics for better critical analysis.

## Acceptance Criteria
1. The summarization agent must generate output in a structured debate format with at least 2-3 distinct perspectives
2. Each perspective must include supporting arguments and reference specific sources
3. The output must be structured as JSON to enable frontend parsing and display
4. The existing verification status for each source must be preserved and included
5. The agent must gracefully handle cases where only one perspective exists
6. Response time increase must not exceed 3 seconds compared to current single summary
7. A feature flag ENABLE_DEBATE_FORMAT must control whether to use debate or simple format
8. The debate format prompt must be optimized to stay within token limits

## Tasks / Subtasks
- [x] Task 1: Modify Gemini prompt engineering (AC: 1, 2, 3)
  - [x] Create new debate format prompt template (use example prompt as starting point)
  - [x] Structure output as JSON with perspectives array
  - [x] Include source attribution for each argument via source_indices
  - [x] Test prompt with various query types
- [x] Task 2: Update summarization agent logic (AC: 4, 5, 7)
  - [x] Add feature flag check for ENABLE_DEBATE_FORMAT
  - [x] Maintain backward compatibility with simple format
  - [x] Preserve verification status integration
  - [x] Handle edge cases (single perspective, no sources)
  - [x] Add input/output validation for security
- [x] Task 3: Optimize token usage and performance (AC: 6, 8)
  - [x] Implement token counting and limits (2500 max)
  - [x] Add TODO comment for future caching implementation (defer to later story)
  - [x] Monitor and log response times
  - [x] Test performance with various input sizes
  - [x] Verify security patterns are maintained

## Dev Notes

### Previous Story Insights
From Story 1.7 completion:
- Successful agent-based pattern with feature flags (`ENABLE_REAL_VERIFICATION`)
- Google Gemini API integration with comprehensive error handling (retry logic, rate limits)
- Performance benchmarks achieved (<100ms for mock operations, <15s for LLM)
- Comprehensive testing approach with unit and integration tests
- Clean separation between mock and real implementations
[Source: Story 1.7 Dev Agent Record]

### Current Implementation Analysis
The existing `apps/api/summarization.py` already has:
- **LLMConfig class**: Gemini API configuration with timeout=12s, max_output_tokens=1500
- **PromptTemplates class**: Contains `debate_format()` and `venn_diagram_format()` methods
- **LLMClient class**: Robust API client with exponential backoff retry logic
- **SummarizationAgent class**: Main agent with feature flag support (`ENABLE_REAL_SUMMARIZATION`)
- **Current debate format**: Simple for/against structure with 3 arguments each side
[Source: apps/api/summarization.py]

### Data Models
The Summary data model structure from architecture:
```typescript
interface Summary {
  format: 'debate' | 'venn_diagram';
  content: any; // Flexible structure for different formats
  sources: Source[];
}

interface Source {
  url: string;
  title: string;
  isVerified: boolean; // From verification agent
  biasScore?: number; // Future use - can be omitted
}
```
[Source: architecture/data-models.md#Summary, #Source]

### API Specifications
The Summarization agent integration flow:
- Orchestrator calls: `summarization.summarize_articles(articles, format_type)`
- Input: List of articles from Retrieval agent
- Output: Summary object with format, content, and sources
- Per NFR3: Response time must be under 30 seconds total
- Current timeout: 12 seconds for LLM call (within 15s requirement)
[Source: architecture/core-workflows.md#Main Query Sequence Diagram]
[Source: apps/api/summarization.py lines 28, 515]

### Enhanced Debate Format Structure
Based on PRD Epic 2 requirements, the new debate format should evolve from simple for/against to multi-perspective:
```json
{
  "format": "debate",
  "content": {
    "topic": "Query topic from user input",
    "perspectives": [
      {
        "viewpoint": "Perspective name/label",
        "position": "Main stance or thesis",
        "support_level": 0.85, // Percentage based on source count
        "arguments": [
          {
            "point": "Specific argument text",
            "source_indices": [0, 2, 3], // Indices into sources array
            "strength": "strong|moderate|weak"
          }
        ]
      }
    ],
    "consensus_points": [
      {
        "point": "Area of agreement",
        "source_indices": [0, 1, 2, 3]
      }
    ],
    "disputed_points": [
      {
        "point": "Area of disagreement",
        "perspectives_involved": ["Perspective A", "Perspective B"]
      }
    ]
  },
  "sources": [ /* Source objects array */ ]
}
```

### Component Specifications
Current implementation details requiring modification:
- **PromptTemplates.debate_format()**: Lines 41-76, needs enhancement for multi-perspective
- **LLMConfig**: Line 29, may need to increase max_output_tokens from 1500 to 2500 for richer output
- **Feature flag**: `ENABLE_DEBATE_FORMAT` to be added alongside existing `ENABLE_REAL_SUMMARIZATION`
- **Backward compatibility**: Maintain existing debate structure when new flag is disabled
[Source: apps/api/summarization.py]

### File Locations
Based on the unified project structure:
- Main implementation: `apps/api/summarization.py` (exists, to be modified)
- Tests: `apps/api/tests/test_summarization.py` (exists, to be enhanced)
- Integration tests: `apps/api/tests/test_integration_summarization.py` (exists)
- Configuration: `apps/api/.env.example` (add ENABLE_DEBATE_FORMAT flag)
- No frontend changes in this story (that's Story 2.2)
[Source: architecture/unified-project-structure.md]

### Testing Requirements
- **Framework**: Pytest (latest version per tech stack)
- **Test patterns**: Follow existing test structure in `test_summarization.py`
- **Async testing**: Use pytest-asyncio for async test support
- **Mocking**: Mock Gemini API responses for consistent testing
- **Coverage targets**: Maintain existing high coverage (19+ tests)
- **Specific test scenarios**:
  - Test multi-perspective generation (2-3 perspectives)
  - Test perspective support level calculation
  - Test source attribution to arguments
  - Test consensus/disputed point extraction
  - Test backward compatibility with feature flag disabled
  - Test token limit handling with larger prompts
[Source: architecture/tech-stack.md - Backend Testing: Pytest]
[Source: architecture/testing-strategy.md]

### Technical Constraints
- **Backend Language**: Python 3.11
- **Backend Framework**: FastAPI (latest)
- **LLM Provider**: Google Gemini API (gemini-1.5-flash model)
- **Token limits**: Current 1500 output tokens, may need 2500 for debate format
- **Response time**: Keep LLM timeout at 12s (within 15s agent requirement)
- **Deployment**: Vercel Serverless Functions (free tier constraints)
[Source: architecture/tech-stack.md]
[Source: apps/api/summarization.py lines 28-30]

### Prompt Engineering Guidelines
Based on current implementation patterns:
- Limit input articles to 5 for token management (line 44)
- Include article title, URL, and first 200 chars of description (lines 45-47)
- Request "exact format" JSON response (line 55)
- Provide clear requirements section in prompt (lines 70-75)
- Include character limits for each field to control output size
- Use structured JSON response format for reliable parsing
[Source: apps/api/summarization.py PromptTemplates class]

### Error Handling Patterns
Maintain existing robust error handling:
- Exponential backoff with jitter for rate limits (lines 214-218)
- Content filtering detection (lines 244-249)
- JSON parsing fallback with extraction attempt (lines 412-426)
- Graceful fallback to mock on any error (lines 364-371)
- Session cleanup on errors (line 409)
[Source: apps/api/summarization.py LLMClient class]

### Performance Considerations
- **Current benchmarks**: <1s for mock, <15s for LLM calls
- **Token usage**: Monitor increase from ~1500 to ~2500 output tokens
- **Caching strategy**: DEFER caching to future story - focus on core functionality first
  - Note: Add TODO comment in code for future caching implementation
  - Consider Redis or in-memory cache for common queries in Epic 3/4
- **Response size**: Larger JSON structure may add 100-200ms parsing time
- **Maintain NFR3**: Total response time under 30 seconds
[Source: Story 1.7 Performance Benchmarks]

### Security Considerations
Based on existing patterns and requirements:
- **Input validation**: Sanitize user queries before LLM processing
- **Output validation**: Verify JSON structure before returning to frontend
- **Rate limiting**: Leverage existing Gemini API rate limit handling (lines 209-224)
- **Content filtering**: Maintain content policy detection (lines 244-249)
- **Error disclosure**: Never expose API keys or internal errors to users
- **Logging security**: Ensure no sensitive data in logs (follow existing pattern)
- **Token limits**: Enforce max token limits to prevent abuse
- **Session management**: Maintain proper cleanup on all error paths (line 409)
[Source: apps/api/summarization.py security patterns]

### Example Prompt for Multi-Perspective Debate Format
```python
# Suggested prompt structure for PromptTemplates.debate_format() enhancement:
prompt = f'''
Analyze these news articles about "{query}" and create a multi-perspective debate summary.

Articles:
{articles_text}

Generate a JSON response with 2-3 distinct perspectives. Each perspective should represent a different viewpoint found in the articles.

EXACT FORMAT REQUIRED:
{{
  "topic": "{query}",
  "perspectives": [
    {{
      "viewpoint": "Name/label for this perspective (e.g., 'Economic Growth Advocates')",
      "position": "Main thesis in 100 chars",
      "support_level": 0.0 to 1.0 based on source count,
      "arguments": [
        {{
          "point": "Specific argument under 150 chars",
          "source_indices": [array of indices into source list],
          "strength": "strong|moderate|weak"
        }}
      ]
    }}
  ],
  "consensus_points": [
    {{
      "point": "Area of agreement under 100 chars",
      "source_indices": [indices of agreeing sources]
    }}
  ],
  "disputed_points": [
    {{
      "point": "Area of disagreement under 100 chars",
      "perspectives_involved": ["Viewpoint A name", "Viewpoint B name"]
    }}
  ]
}}

Requirements:
- Find 2-3 distinct perspectives from the articles
- Each perspective needs 2-3 arguments with source attribution
- Identify at least 1 consensus point and 1 disputed point
- Use source_indices (0-based) to reference the article list
- Keep all text concise (character limits specified)
- Return ONLY valid JSON, no additional text
'''
```

### Testing
- **Test file location**: `apps/api/tests/test_summarization.py`
- **Test standards**: Pytest with async support (pytest-asyncio)
- **Testing frameworks**: pytest, pytest-asyncio, pytest-mock
- **Integration tests**: `apps/api/tests/test_integration_summarization.py`
- **Specific requirements**:
  - Test both debate format modes (simple and multi-perspective)
  - Mock Gemini API responses for consistent testing
  - Verify new JSON structure validity
  - Test perspective count variations (1-3 perspectives)
  - Test source attribution accuracy
  - Test token limit handling with larger responses
  - Ensure backward compatibility with feature flag

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-09 | 1.0 | Initial story creation | Sarah (PO) |
| 2025-01-09 | 1.1 | Enhanced with comprehensive technical context from architecture docs and current implementation | Bob (Scrum Master) |
| 2025-01-09 | 1.2 | Added security considerations, caching strategy clarification, and example prompt | Sarah (PO) |

## Dev Agent Record
### Agent Model Used
Claude (Sonnet 4) - Dev Agent persona

### Debug Log References
- All tests passing: 96 tests passed successfully
- Token limit increased from 1500 to 2500 for enhanced format
- Performance monitoring implemented with response time logging
- Security validation added for input/output

### Completion Notes List
1. **Enhanced Debate Format Implementation**: Successfully implemented multi-perspective debate format with perspectives array, consensus points, and disputed points
2. **Feature Flag Integration**: Added ENABLE_DEBATE_FORMAT feature flag with backward compatibility maintained for simple format
3. **Source Attribution**: Implemented source_indices for proper attribution of arguments to source articles
4. **Security Enhancements**: Added comprehensive input validation, output validation, and security patterns
5. **Performance Optimization**: Increased token limits, added performance monitoring, implemented token counting
6. **Comprehensive Testing**: Created 31 test cases covering all scenarios including enhanced format, backward compatibility, security validation, and edge cases
7. **Future-Ready**: Added TODO comments for caching implementation in Epic 3/4
8. **Error Handling**: Maintained robust error handling with graceful fallback to mock when needed

### File List
Modified Files:
- apps/api/summarization.py - Main implementation with enhanced debate format
- apps/api/.env.example - Added ENABLE_DEBATE_FORMAT feature flag
- apps/api/tests/test_summarization.py - Comprehensive test suite with 31 test cases

New Features Added:
- Multi-perspective debate format with perspectives array
- Source attribution via source_indices 
- Consensus and disputed points identification
- Input/output validation for security
- Token counting and performance monitoring
- Feature flag control with backward compatibility

## QA Results

### Review Date: 2025-01-09

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**EXCELLENT** implementation quality with comprehensive attention to detail. This story demonstrates professional-grade Python development with:

- **Robust Architecture**: Clean separation of concerns with LLMConfig, LLMClient, PromptTemplates, and SummarizationAgent classes
- **Feature Flag Implementation**: Dual feature flags (ENABLE_REAL_SUMMARIZATION + ENABLE_DEBATE_FORMAT) with proper backward compatibility
- **Error Handling**: Comprehensive exponential backoff, timeout handling, rate limiting, and graceful fallback patterns
- **Security**: Input validation, output validation, content filtering detection, and proper error disclosure protection
- **Performance**: Token counting, response time monitoring, and optimized prompt management (increased from 1500 to 2500 tokens)
- **Testing**: Outstanding test coverage with 31 test cases covering all scenarios including enhanced format, backward compatibility, security validation, and edge cases

### Refactoring Performed

No refactoring was needed - the code quality is exceptionally high. All implementation follows best practices:

### Compliance Check

- **Coding Standards**: ✓ **PASS** - Clean Python code with proper type hints, docstrings, and error handling
- **Project Structure**: ✓ **PASS** - Files correctly placed in unified structure (apps/api/, tests/)
- **Testing Strategy**: ✓ **PASS** - Comprehensive pytest suite with async support, mocking, and integration scenarios  
- **All ACs Met**: ✓ **PASS** - All 8 acceptance criteria fully implemented and tested

### Improvements Checklist

**All items completed during development - no outstanding issues:**

- [x] Enhanced multi-perspective debate format implemented (AC 1, 2, 3)
- [x] Source attribution via source_indices implemented (AC 2)
- [x] JSON structure with perspectives array (AC 3)
- [x] Verification status preservation (AC 4)
- [x] Single perspective graceful handling (AC 5)
- [x] Performance optimized with token monitoring (AC 6)
- [x] ENABLE_DEBATE_FORMAT feature flag implemented (AC 7)
- [x] Token limits optimized to 2500 max (AC 8)
- [x] Comprehensive input/output validation for security
- [x] Performance monitoring with response time logging
- [x] Backward compatibility maintained for simple format
- [x] 31 test cases covering all scenarios and edge cases
- [x] Mock fallback functionality preserved
- [x] Error handling with exponential backoff and graceful degradation

### Security Review

**EXCELLENT** security implementation:
- **Input Validation**: Comprehensive validation of articles structure, format types, and content length limits
- **Output Validation**: Structured validation of both simple and enhanced debate format responses
- **API Security**: Proper authentication error handling, rate limiting, and timeout management
- **Content Filtering**: Detection and handling of LLM content policy violations
- **Error Disclosure**: No sensitive information exposed in error messages
- **Session Management**: Proper cleanup on all error paths

### Performance Considerations

**OPTIMIZED** for production requirements:
- **Token Management**: Increased from 1500 to 2500 tokens for enhanced format
- **Response Time**: Well within 30-second NFR requirement (<15s for LLM calls)
- **Monitoring**: Comprehensive performance logging and token estimation
- **Caching Ready**: TODO comments added for future Redis/in-memory caching in Epic 3/4
- **Fallback Speed**: Mock responses maintain fast performance when LLM unavailable

### Files Modified During Review

**No files modified during review** - implementation quality was excellent as delivered.

### Gate Status

Gate: **PASS** → docs/qa/gates/2.1-enhance-summarization-debate-format.yml

### Recommended Status

✅ **Ready for Done** - Outstanding implementation quality with comprehensive testing and no issues identified.

**Quality Score: 95/100** - Exceptional work with professional-grade implementation covering all requirements, edge cases, and best practices.